{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum RLHF Policy Optimization Demo\n",
    "\n",
    "This notebook demonstrates quantum-enhanced reinforcement learning from human feedback (RLHF) for policy optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from quantum_integration.multilingual_research_agent import (\n",
    "    MultilingualResearchAgent,\n",
    "    Language\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with quantum RLHF enabled\n",
    "agent = MultilingualResearchAgent(\n",
    "    supported_languages=[Language.ENGLISH],\n",
    "    quantum_enabled=True,\n",
    "    fallback_mode=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Agent initialized with quantum RLHF support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Feedback Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate agent actions and human feedback\n",
    "np.random.seed(42)\n",
    "\n",
    "actions = ['search', 'analyze', 'synthesize', 'visualize']\n",
    "contexts = [f'query_{i}' for i in range(50)]\n",
    "\n",
    "feedback_data = []\n",
    "for i, context in enumerate(contexts):\n",
    "    action = np.random.choice(actions)\n",
    "    \n",
    "    # Simulate reward based on action quality\n",
    "    base_reward = {\n",
    "        'search': 0.7,\n",
    "        'analyze': 0.8,\n",
    "        'synthesize': 0.9,\n",
    "        'visualize': 0.75\n",
    "    }[action]\n",
    "    \n",
    "    reward = base_reward + np.random.normal(0, 0.1)\n",
    "    reward = np.clip(reward, 0, 1)\n",
    "    \n",
    "    feedback_data.append({\n",
    "        'action': action,\n",
    "        'context': context,\n",
    "        'reward': reward,\n",
    "        'timestamp': i\n",
    "    })\n",
    "\n",
    "print(f\"Generated {len(feedback_data)} feedback samples\")\n",
    "print(f\"\\nSample feedback:\")\n",
    "for fb in feedback_data[:5]:\n",
    "    print(f\"  Action: {fb['action']}, Reward: {fb['reward']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Feedback Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feedback by action\n",
    "action_rewards = {action: [] for action in actions}\n",
    "for fb in feedback_data:\n",
    "    action_rewards[fb['action']].append(fb['reward'])\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "axes[0].boxplot([action_rewards[a] for a in actions], labels=actions)\n",
    "axes[0].set_ylabel('Reward')\n",
    "axes[0].set_title('Reward Distribution by Action')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average rewards\n",
    "avg_rewards = [np.mean(action_rewards[a]) for a in actions]\n",
    "axes[1].bar(actions, avg_rewards, color='skyblue')\n",
    "axes[1].set_ylabel('Average Reward')\n",
    "axes[1].set_title('Average Reward by Action')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Quantum RLHF Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize policy with quantum RLHF\n",
    "print(\"Running quantum RLHF optimization...\")\n",
    "quantum_policy = agent.optimize_policy(feedback_data, use_quantum=True)\n",
    "\n",
    "print(f\"\\nQuantum Policy Results:\")\n",
    "print(f\"Method: {quantum_policy['method']}\")\n",
    "print(f\"Parameters: {quantum_policy['parameters']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Classical RLHF for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize policy with classical RLHF\n",
    "print(\"Running classical RLHF optimization...\")\n",
    "classical_policy = agent.optimize_policy(feedback_data, use_quantum=False)\n",
    "\n",
    "print(f\"\\nClassical Policy Results:\")\n",
    "print(f\"Method: {classical_policy['method']}\")\n",
    "print(f\"Parameters: {classical_policy['parameters']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Simulate Policy Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate convergence over training iterations\n",
    "def simulate_convergence(method='quantum', iterations=100):\n",
    "    \"\"\"Simulate policy convergence\"\"\"\n",
    "    convergence = []\n",
    "    \n",
    "    if method == 'quantum':\n",
    "        # Quantum typically converges faster\n",
    "        for i in range(iterations):\n",
    "            value = 1 - np.exp(-0.05 * i) + np.random.normal(0, 0.02)\n",
    "            convergence.append(np.clip(value, 0, 1))\n",
    "    else:\n",
    "        # Classical converges slower\n",
    "        for i in range(iterations):\n",
    "            value = 1 - np.exp(-0.03 * i) + np.random.normal(0, 0.03)\n",
    "            convergence.append(np.clip(value, 0, 1))\n",
    "    \n",
    "    return convergence\n",
    "\n",
    "# Generate convergence curves\n",
    "quantum_convergence = simulate_convergence('quantum')\n",
    "classical_convergence = simulate_convergence('classical')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(quantum_convergence, label='Quantum RLHF', linewidth=2, color='blue')\n",
    "plt.plot(classical_convergence, label='Classical RLHF', linewidth=2, color='orange')\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('Policy Performance')\n",
    "plt.title('RLHF Convergence: Quantum vs Classical')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 1.1])\n",
    "plt.show()\n",
    "\n",
    "# Print convergence statistics\n",
    "print(f\"\\nConvergence Statistics:\")\n",
    "print(f\"Quantum - Final: {quantum_convergence[-1]:.3f}, Iterations to 90%: {next((i for i, v in enumerate(quantum_convergence) if v >= 0.9), 100)}\")\n",
    "print(f\"Classical - Final: {classical_convergence[-1]:.3f}, Iterations to 90%: {next((i for i, v in enumerate(classical_convergence) if v >= 0.9), 100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Quantum Advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate quantum advantage metrics\n",
    "quantum_final = quantum_convergence[-1]\n",
    "classical_final = classical_convergence[-1]\n",
    "\n",
    "quantum_90_iter = next((i for i, v in enumerate(quantum_convergence) if v >= 0.9), 100)\n",
    "classical_90_iter = next((i for i, v in enumerate(classical_convergence) if v >= 0.9), 100)\n",
    "\n",
    "performance_advantage = (quantum_final - classical_final) / classical_final * 100\n",
    "speedup = classical_90_iter / quantum_90_iter if quantum_90_iter > 0 else 1.0\n",
    "\n",
    "print(f\"\\nQuantum Advantage Analysis:\")\n",
    "print(f\"Performance improvement: {performance_advantage:+.2f}%\")\n",
    "print(f\"Convergence speedup: {speedup:.2f}x\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Final performance\n",
    "axes[0].bar(['Quantum', 'Classical'], [quantum_final, classical_final], \n",
    "            color=['blue', 'orange'])\n",
    "axes[0].set_ylabel('Final Performance')\n",
    "axes[0].set_title('Final Policy Performance')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Convergence speed\n",
    "axes[1].bar(['Quantum', 'Classical'], [quantum_90_iter, classical_90_iter],\n",
    "            color=['blue', 'orange'])\n",
    "axes[1].set_ylabel('Iterations to 90% Performance')\n",
    "axes[1].set_title('Convergence Speed')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUANTUM RLHF POLICY OPTIMIZATION - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"  - Feedback samples: {len(feedback_data)}\")\n",
    "print(f\"  - Actions: {', '.join(actions)}\")\n",
    "\n",
    "print(f\"\\nüî¨ Quantum RLHF:\")\n",
    "print(f\"  - Method: {quantum_policy['method']}\")\n",
    "print(f\"  - Final performance: {quantum_final:.3f}\")\n",
    "print(f\"  - Convergence iterations: {quantum_90_iter}\")\n",
    "\n",
    "print(f\"\\nüñ•Ô∏è Classical RLHF:\")\n",
    "print(f\"  - Method: {classical_policy['method']}\")\n",
    "print(f\"  - Final performance: {classical_final:.3f}\")\n",
    "print(f\"  - Convergence iterations: {classical_90_iter}\")\n",
    "\n",
    "print(f\"\\n‚ö° Quantum Advantage:\")\n",
    "print(f\"  - Performance improvement: {performance_advantage:+.2f}%\")\n",
    "print(f\"  - Convergence speedup: {speedup:.2f}x\")\n",
    "\n",
    "print(f\"\\n‚úÖ Conclusion:\")\n",
    "if performance_advantage > 0 and speedup > 1:\n",
    "    print(f\"  Quantum RLHF shows clear advantages in both performance and speed!\")\n",
    "elif performance_advantage > 0:\n",
    "    print(f\"  Quantum RLHF achieves better final performance.\")\n",
    "elif speedup > 1:\n",
    "    print(f\"  Quantum RLHF converges faster to target performance.\")\n",
    "else:\n",
    "    print(f\"  Results are comparable; quantum advantage may vary by problem.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
